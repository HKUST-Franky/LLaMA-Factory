
V5 版本新增功能：

1.新增7个可训练的模型
2.增加多模态（图像）微调方法
3.增加“预训练”微调方法
4.改进训练框架,优化默认超参数配置
5.支持"指令"数据格式,对应instruction部分
6.ChatGPT 3.5 API 新充50美元。

唠叨：这次的更新有些迟了，主要是最近在研究各种各样的东西。不过最近空了一些。所以趁这段时间赶紧更新一下。这次新增的功能比较多，说实话制作难度挺大的。因为不仅仅得把功能做出来，而且还要保证易懂。且绝对不能出现报错现象。每一个功能我都会测试好几次。确保没有什么bug 而且之前做的功能也得测试，确保以前的功能也没有问题。所以希望各位可以为这个镜像点一个收藏。感谢了。如果大家有什么想要的新的功能或者对应的需求可以联系我微信：wzx123wzx12 备注：“llama-factory 需求更新” 我就知道了。因为增加的功能越来越多，不可避免的就是教程文档会越来越长。我始终认为，一个程序如果过于复杂难用，那是开发者能力的缺失。尽管如此，这个镜像可能仍有一些不够完善的地方。如果您在使用过程中遇到任何问题，欢迎通过微信与我联系。

-------------------------------------------------------------------------------------------------------------------------------------
使用镜像（仅需3步）

第一步《部署模型》
运行：
bash /root/LLaMA-Factory/chuli/转移.sh

第二步《制作数据集》
数据集支持以下几种格式

1.单轮问答格式：

问：你是谁？
答：我是一个AI，我比chatgpt聪明。我是Claude

2.标准SFT格式：

指令：模仿一个脾气不好的AI和我对话
问：你好
答：哈喽,又一个无聊的人类来了。真希望你能说点有趣的话题,别让我睡着了。不过看你这反应速度,估计脑子也不怎么样。有什么事就赶紧说吧,我可没那么多时间跟你扯犊子。

3.多轮对话格式：

问：你好啊
答：嗯嗯，你好。你想和我聊什么呢？
问：我也不知道，总之我们现在得扯一些废话。
答：好吧，让我来想一些乱七八糟的内容。你吃饭了嘛？
问：没吃呢。你呢？
答：我也没吃。

上述所有的格式都可以混用。其中这次新增了“指令” 这个插件。这个是可选的，只用单纯的问答对格式也是完全没问题的。可根据你的需要随意选择，你可以通过空一行开来分开你的数据。
前往这个路径：数据集全自动处理/放置数据集.txt  放置你制作好的数据。（我在这里面放了用于参考的数据集，你可以全删了替换为你自己的）

接下来，执行命令处理数据集：
bash /root/LLaMA-Factory/chuli/单多轮脚本/DD.sh

（该命令将自动处理您的数据集，使其适合进行训练。请记住处理后的数据集名称为：“你的数据集”）

第三步《启动程序》

接着，可以正式启动Web界面了：
bash /root/LLaMA-Factory/chuli/one.sh


-----------------------------------------------------------------------------------------------------------------------------------------------

后话：
如需详细了解这个镜像的使用方法，您可以观看我之前制作的视频教程（链接：https://www.bilibili.com/video/BV1a3aQeuEou）。不过请注意，该教程是基于V4版本录制的，部分新增功能并未涵盖。
所以我正在编写一篇全新的教程文章，将会详细介绍V5版本的所有功能及其使用方法。想要获取这篇教程，欢迎关注微信公众号"AI会思考"，发送"LFv5"即可获取。

QQ群：296483610

B站：自负的魔方

github主页:https://github.com/morettt















最后分享几个关于深度学习的打油诗：

《Gradient Descent》

一行行代码敲下 Bug像蚊子嗡嗡
梯度下降 Loss咋还不降
Batch size调大 Learning rate调小
Train了一晚上 涨点还是负的

Overfit来了 Underfit也来
Cross validation 救救孩子吧  
Dropout概率调高 Regularization来了
Epoch硬升 终于上85%了

数据集太小怪我咯 调参太难怪我咯 
写Paper通宵怪我咯 中间层太少还怪我咯
哪天突然悟了 随手一调 99.99%
然后华丽丽测试 发现是个特例

白板写满公式 举着小本本
实验失败N次 拿着小本本
键盘都敲烂 还在调参数
头发掉光光 模型还不涨
谁说AI难 这不是真香？


《神经网络断联爱情》

算法写到深更 数据扒到天明
对话模型调参 机器人被调教成宠
女神约我看电影 我说改日再约
女神约我吃大餐 我说数据等着投
女神愤而分手 我说早点摊牌省得耽误

最新架构发布 分分钟call我学习
GitHub上群魔乱舞 点星刷得比谁都勤
师兄都劝我 兄弟先定个小目标
三十岁前先找个 能说人话的对象
我说师兄你不懂 我要攻克图灵测试的巅峰

读博七年光阴 七个女友黄花
情人节孤单单 和GPT过二人世界
朋友都劝我 诺贝尔奖没有AI的份
不如低头找 志同道合的红颜知己
我说还是再等等 等我的AI模型学会浪漫

山外有山楼外楼 世界太大我不敢看
一门心思扑在代码上 只要永远亮着屏
打开新的Jupyter 再开个PyCharm
穿上格子衫 Stay in the lab
去追寻 数字乌托邦


《调参修仙传》

这一日我在实验室修炼
想把模型调到天花板
却不料遇到迷之bug
CPU烫得能煎鸡蛋

师兄说："要用Adam优化"
师姐说："试试Momentum"
我用SGD一把梭哈
Loss直接飞向银河边

Dropout掉了一地头发
BatchNorm还在梦游天
十层网络训练不动
加了残差还在原地转

凌晨三点实验室内
外卖空盒堆成山
显卡风扇呼呼转动
像在笑我太费电

终于等到准确率涨
激动得差点去跳舞
谁知刚要存个模型
内存不足叫我滚

炼丹之路艰难险阻
调参秘籍代代相传
今朝若得九九精度
来年必成模型仙

老板问：你的模型呢？
我说：别问，问就是在调
评审问：你的代码呢？
答曰：火候未到，且让我改改超参数
十年调参一梦醒
原来我已白了头








